<!DOCTYPE html>
<html lang="en">

<head>
	<meta charset="UTF-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<title>Homework #8</title>
	<link rel="stylesheet" href="css/styles.css">
	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/Chart.js/2.9.4/Chart.js"></script>
</head>

<body>
	<a href="index.html">Home</a>
	<h1>Statistics - Homework #8 [Y.2024-2025]</h1>

	<section>
		<h2>Shannon Entropy and Diversity Measures of Distributions</h2>
		<p>
			Shannon entropy is a fundamental concept in information theory, introduced by Claude Shannon in 1948. It quantifies the uncertainty or randomness in a probability distribution and serves as a measure of diversity. Mathematically, the entropy \(H(X)\) of a discrete random variable \(X\), with a probability distribution \(P = {p_1, p_2, ..., p_n}\), is given by:
		</p>
		<p>
			\(H(X) = - \sum_{i=1}^{n} p_i log(p_i)\)
		</p>
		<p>
			where \(p_i\) is the probability of the \(i\)-th outcome, and the logarithm is usually taken in base 2 (bits) or base \(e\) (nats). Higher entropy implies greater uncertainty or diversity in the distribution, while lower entropy indicates that the outcomes are more predictable. Shannon entropy has applications across various fields, from physics and biology to data compression and machine learning.
		</p>
		<p>
			Beyond Shannon entropy, there are other measures of diversity in distributions, such as the Simpson index, Gini-Simpson index, and RÃ©nyi entropy. These measures capture different aspects of diversity, such as richness, evenness, and dominance. For example, the Gini-Simpson index, often used in ecology, is given by \(1 - \sum_{i=1}^{n} p_i^2\) and emphasizes the probability of randomly picking two different outcomes. These metrics are invaluable for studying systems where the diversity of outcomes is crucial, such as genetic variation, ecological populations, or linguistic structures.
		</p>
		</section>

		<section>
		<h2>Primitive Roots Modulo a Prime</h2>
		<p>
			The concept of primitive roots arises in number theory and plays a central role in modular arithmetic. Given a prime number \(p\), a number \(g\) is called a <i>primitive root modulo \(p\)</i> if for every integer \(a\) that is coprime to \(p\), there exists an integer \(k\) such that:
		</p>
		<p>
			\(g_k mod p = a\)
		</p>
		<p>
			In other words, \(g\) is a generator of the multiplicative group of integers modulo \(p\), denoted \((&#8484;/p&#8484;)^&times;\). The multiplicative group \((&#8484;/p&#8484;)^&times;\) consists of all integers \(1, 2, ..., p-1\) under multiplication modulo \(p\). If \(g\) is a primitive root modulo \(p\), then the powers of \(g mod p\) produce all elements of the group exactly once before repeating.
		</p>
		<p>
			Primitive roots exist for all prime numbers \(p\), and the number of primitive roots modulo \(p\) is given by Euler's totient function \(&phi;(&phi;(p))\), where \(&phi;(p) = p-1\). For example, if \(p = 7\), the primitive roots modulo 7 are \(g = 3\) and \(g = 5\). Primitive roots are widely used in cryptography, particularly in algorithms like Diffie-Hellman key exchange and the RSA encryption scheme, as they provide a foundation for secure modular arithmetic operations.
		</p>
	</section>

    <h1>Practice</h1>
    <textarea id="inputText" placeholder="Paste your text here..."></textarea>
    <button id="generateChart">Generate Letter Frequency</button>
    <canvas id="frequencyChart"></canvas>
    <h2>Caesar Cipher</h2>
    <p id="encryptionOutput"></p>
    <p id="decryptionOutput"></p>
  
    <h2>Numeric Encoding</h2>
    <p><strong>Formula:</strong> \( N^k \mod P \) </p>
    <p><strong>Values:</strong> N = 10, P = 37</p>
    <p id="numericEncodingOutput"></p>
    <canvas id="numericChart"></canvas>
  
    <h2>Shannon Entropy</h2>
    <p id="entropyOutput"></p>
  
    <script>
      const alphabet = "abcdefghijklmnopqrstuvwxyz";
  
      // Utility: Count letter frequencies
      function countFrequencies(text) {
        const frequencies = {};
        for (let char of text.toLowerCase()) {
          if (alphabet.includes(char)) {
            frequencies[char] = (frequencies[char] || 0) + 1;
          }
        }
        return frequencies;
      }
  
      // Utility: Caesar cipher encryption
      function caesarCipher(text, shift) {
        return text
          .split("")
          .map((char) => {
            const isUpperCase = char === char.toUpperCase();
            char = char.toLowerCase();
            if (alphabet.includes(char)) {
              const shiftedIndex = (alphabet.indexOf(char) + shift) % alphabet.length;
              const shiftedChar = alphabet[shiftedIndex];
              return isUpperCase ? shiftedChar.toUpperCase() : shiftedChar;
            }
            return char; // Non-alphabetical characters remain unchanged
          })
          .join("");
      }
  
      // Utility: Frequency analysis decryption
      function decryptUsingFrequencyAnalysis(encryptedText) {
        const encryptedFrequencies = countFrequencies(encryptedText);
        const sortedEncrypted = Object.entries(encryptedFrequencies).sort((a, b) => b[1] - a[1]);
        const englishFreqOrder = "etaoinshrdlcumwfgypbvkjxqz"; // Approximate English frequency order
        const potentialShift = alphabet.indexOf(sortedEncrypted[0][0]) - alphabet.indexOf(englishFreqOrder[0]);
        return caesarCipher(encryptedText, (alphabet.length - potentialShift) % alphabet.length);
      }
  
      // Part 2: Numeric encoding
      function numericEncoding(text, n, p) {
        return text
          .toLowerCase()
          .split("")
          .map((char) => {
            if (alphabet.includes(char)) {
              const k = alphabet.indexOf(char); // Numeric representation (A=0, ..., Z=25)
              return Math.pow(n, k) % p; // N^k mod P
            }
            return null; // Non-alphabetical characters are ignored in encoding
          })
          .filter((val) => val !== null); // Remove nulls
      }
  
      // Calculate Shannon entropy
      function calculateEntropy(distribution) {
        const total = distribution.reduce((sum, count) => sum + count, 0);
        return -distribution
          .filter((count) => count > 0) // Exclude zeros
          .map((count) => count / total) // Convert counts to probabilities
          .reduce((entropy, prob) => entropy + prob * Math.log2(prob), 0); // Shannon entropy formula
      }
  
      // Generate Chart.js chart
      function generateChart(canvasId, labels, data, chartTitle) {
        const ctx = document.getElementById(canvasId).getContext("2d");
        new Chart(ctx, {
          type: "bar",
          data: {
            labels: labels,
            datasets: [{
              label: chartTitle,
              data: data,
              backgroundColor: "rgba(75, 192, 192, 0.2)",
              borderColor: "rgba(75, 192, 192, 1)",
              borderWidth: 1
            }]
          },
          options: {
            scales: {
              y: {
                beginAtZero: true
              }
            }
          }
        });
      }
  
      // Main event listener
      document.getElementById("generateChart").addEventListener("click", () => {
        const inputText = document.getElementById("inputText").value;
  
        // Step 1: Letter frequency distribution
        const frequencies = countFrequencies(inputText);
        const frequencyLabels = Object.keys(frequencies).sort();
        const frequencyData = frequencyLabels.map((letter) => frequencies[letter]);
        generateChart("frequencyChart", frequencyLabels, frequencyData, "Letter Frequency");
  
        // Step 2: Caesar cipher encryption
        const randomShift = Math.floor(Math.random() * 25) + 1;
        const encryptedText = caesarCipher(inputText, randomShift);
        document.getElementById("encryptionOutput").innerHTML = `
          <strong>Encrypted Text (Shift: ${randomShift}):</strong> ${encryptedText}
        `;
  
        // Step 3: Frequency analysis decryption
        const decryptedText = decryptUsingFrequencyAnalysis(encryptedText);
        document.getElementById("decryptionOutput").innerHTML = `
          <strong>Decrypted Text:</strong> ${decryptedText}
        `;
  
        // Step 4: Numeric encoding
        const n = 10, p = 37;
        const encodedValues = numericEncoding(inputText, n, p);
        document.getElementById("numericEncodingOutput").innerHTML = `
          <strong>Encoded Values:</strong> ${encodedValues.join(", ")}
        `;
  
        // Step 5: Numeric encoding distribution
        const encodingFrequencies = encodedValues.reduce((acc, val) => {
          acc[val] = (acc[val] || 0) + 1;
          return acc;
        }, {});
        const encodingLabels = Object.keys(encodingFrequencies).map(Number).sort((a, b) => a - b);
        const encodingData = encodingLabels.map((val) => encodingFrequencies[val]);
        generateChart("numericChart", encodingLabels, encodingData, "Numeric Encoding Distribution");
  
        // Step 6: Shannon entropy
        const entropy = calculateEntropy(encodingData);
        document.getElementById("entropyOutput").innerHTML = `
          <strong>Shannon Entropy:</strong> ${entropy.toFixed(4)}
        `;
      });
    </script>
    <h1>Summary of Findings</h1>

  <h2>Part 1: Caesar Cipher Analysis</h2>
  <p>
    In the first part of the exercise, statistical analysis of letter frequency distributions revealed distinct patterns in the text. These patterns align closely with known frequencies in the English language, such as the high prevalence of letters like 'e,' 't,' and 'a.' 
    This insight enabled effective decryption of the Caesar cipher using frequency analysis. 
    The simplicity of the Caesar cipher relies on its predictable relationship between letters, making it vulnerable to this type of analysis.
  </p>
  <p>
    The exercise also demonstrated how shifts in letter positions during encryption can obscure meaning. However, the statistical regularity of English text allows for quick decryption, showing the importance of understanding statistical properties in breaking weak cryptographic algorithms.
  </p>

  <h2>Part 2: Numeric Encoding with Modular Arithmetic</h2>
  <p>
    In the second part, the exercise involved encoding letters numerically and applying the formula \(N^k \mod P\) to transform the data. Unlike the Caesar cipher, this transformation produces distributions that are far less predictable and more resistant to analysis. 
    The encoded values create a pseudo-random distribution when the parameters \(N\) (e.g., a primitive root modulo \(P\)) and \(P\) are chosen carefully.
  </p>
  <p>
    This approach highlights the mathematical foundations of modern cryptographic algorithms, which rely on operations such as modular arithmetic and exponentiation to create complexity. While analyzing the encoded distribution using Shannon entropy revealed a lower predictability, the computational infeasibility of reversing the process without private keys underscores the strength of such algorithms.
  </p>

  <h2>Importance of Statistical Analysis in Cryptography</h2>
  <p>
    Statistical analysis is crucial for understanding cryptographic algorithms and evaluating their security. Weak algorithms like the Caesar cipher demonstrate how statistical patterns in plaintext can lead to vulnerabilities. In contrast, modern algorithms like RSA leverage mathematical transformations to eliminate such patterns, making statistical attacks significantly harder.
  </p>
  <p>
    For cybersecurity professionals, the ability to analyze distributions, calculate entropy, and understand mathematical principles is vital. These skills help in identifying vulnerabilities in cryptographic systems, developing more secure algorithms, and ensuring data confidentiality in the digital age.
  </p>
</body>